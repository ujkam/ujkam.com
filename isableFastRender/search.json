[{"categories":null,"content":"When conceptualizing and implementing a predictive maintenance project, it can be hard to grasp the entire chain of people and technology needed for success. In this post I’ll try to break down all the people and teams needed for this type of project, and also provide insight into how these people work together.\nAn alternate title I had for this post was “How much is my predictive maintenance program really going to cost?” The goal is not give a dollar amount, but really to understand how all the different people and pieces fit together so you can fully conceptualize the scope of one of these programs.\nLets start with the assumptions. This post is focused on projects that actually make it to “production”, which means they generate some real financial benefit. R\u0026D work, proof-of-concepts, exploratory activities, etc. are all valuable, but they have working patterns that can vary quite a bit from what I’m describing here.\nThis post focuses on predictive maintenance projects addressing specific use cases, not general purpose solutions. A general purpose solution is something like an anomaly detection SaaS trained on a variety of machine data, but it needs to be customized for each use case. Developing and using this type of SaaS is different from what this post is addressing. I’ll provide an example of what I mean by “specific use case” soon.\nI’ve also pointed at a “Data Scientist” as the person who implements these projects, but more realistically it could be any person with data and software skills. Job titles mean different things at different companies and there nothing stopping a Data Analyst or Data Engineer from doing these things. So don’t take my using the words “Data Scientist” as some sort of gold standard for who needs to do this work.\nFor the sake of simplicity, let’s split these projects into two types, low and high complexity projects. “Complexity” refers to the number of people and the the computing environment used to develop and deploy a model, and also to evaluate and act on Alarms (the model inference/predictions). Complexity is not referring to complexity of the actual model.\nA Low complexity project consists of a smaller group of people working on a focused use case, with a more manual process around evaluating and acting on alarms. A high complexity project has a lot more people and technology in the development of use cases and evaluating and acting on alarms. Again, it’s not about the actual model, but everything around the model.\nA Low Complexity Project Let’s start with an example. A Subject Matter Expert (SME) identifies a machine issue (a failure mode) they’d like to predict, and proposes rules that could be used to predict this. They work with a Data Scientist to codify (e.g. using Python) and test those rules, and both of them work together to evaluate and adjust those rules to improve predictive performance. Once they are both happy with the results, the Data Scientist deploys the “production” script into a system that runs it once a day. After the script runs, it emails a list (also known as alarms) of all systems that violated those rules to the SME. When the SME gets the email, they evaluate the alarms by investigating the machine sensor data. For any systems with genuine issues, they manually create a service ticket for the relevant system so somebody can intervene.\nThere are a few points I’d like to highlight.\nThe “deployment” system could be as simple as a laptop running the script once a day using the operating system scheduler. Granted, with cloud services everywhere, a laptop is less likely than the cloud equivalent (e.g. a small cloud VM). The “production” script basically fetches some data (from a database, from a file), does something with this data, and sends an email. It’s not necessarily built with software engineering or DevOps best practices, logging, or even very tolerant of failure. The impact of the script failing is low. If the laptop turns off and the script doesn’t run for a few days, only the SME will notice and care. So there limited need to harden the script and the deployment environment to prevent these types of issues. The “monitoring” is done by the SME, and they ensure bad alarms don’t reach the service personnel. If the SME goes on holiday for two weeks and nobody looks at the alarms, it doesn’t result in a organizational or contractual issue. The complexity of this project is low because the organizational overhead is low. The entire process fits on a single slide, and the entire project is executed by two people. Any support needed from other people is minor (e.g. somebody in an IT role needs to allow the script to send emails).\nScaling This One major challenge is this entire process being bottlenecked by these two people. A few sick days means alarms are no longer checked or tracked. If either of them have other job responsibilities taking up a lot of mental space, they may not remember, or have the time, to check the alarms or if the script is still running.\nOnce you get past a handful of use cases, it’s difficult to sustain this low complexity model. If nothing else, the Service organization is going to ask why SMEs are suddenly creating service tickets based on a “predictive model”, and will also ask for a business justification for this new way of working. The SME’s management structure will need to step in. When two different parts of an organization need to work together, a process will be need to be defined.\nUnless your goal is to stay small, trying to scale and resolve these types of issues is what changes this from a low complexity project to a high complexity one. To scale, you’ll need to bring in other people and other groups and find a way for them to work together.\nA High Complexity Project/Program Here’s an example reflecting a more complex situation. A team of SMEs come up with failure modes they’d like to predict, and each of these failure modes is given a business value to help prioritize which to work on. A team of Data people work with them to build resilient data pipelines and rules/models. As part of the development process, every project is tested for a fixed period of time, and the quality of alarms are summarized using predefined metrics. Only the models passing a defined target can then be moved to a standardized and supported production system. To move the project to production, there are requirements the code has to meet for logging, input/output, and so it is resilient to upstream/downstream issues. Once the project is in production, the alarms go directly to another business team who monitor and act on them. The data, SMEs, and business teams are also all involved in monitoring alarm quality over time to catch any changes in behavior.\nFor the sake of clarity, let’s define all the teams of people above.\nThe SME Team: The fundamental goal of this team is to come up with ideas or use cases, and to provide support when the Data team builds solutions. This is typically a cross-functional team and they may only do this part time (e.g. 90% of their job is something else). So from an organizational perspective, they report to different managers and have different overall goals for their jobs. They may have mixed backgrounds and motivations and aren’t always aligned with each other.\nFor example, one member of this team could be from Customer Service, with a focus on issues directly affecting customers. Another person could be from Engineering, and they are focused on hardware issues causing machine damage, even if the occurrence of these issues are low. A third person might be very finance focused and interested in issues which cost the company the most money, regardless of customer impact. Keep in mind a company can have multiple products, so you might have multiple SME teams competing to get their voices heard and use cases prioritized.\nThe Data Team: This team will have three core functions. 1). Doing data things such as building data pipelines, analyzing it, building models, etc. 2). Building/Maintaining a development and production infrastructure. 3). Ensuring the outputs of the previous two items are useful to other teams. This could include dashboards so alarms can quickly be checked and validated against other data (e.g. machine sensor data), an app to track and handle alarms, documentation, etc.\nPart of this is working with other teams who own the upstream systems (e.g. the raw machine data), and also the downstream systems (e.g. the service ticketing system) so all the production infrastructure can talk to these systems in an efficient and standardized way. The infrastructure also needs to be resilient to failures, so any upstream and downstream issues do not result in the production system crashing and losing alarms, or worse, generating incorrect alarms.\nThe Service and Alarm team: After the alarms are generated, someone has do something with them. This could be a single specialized team, or an entire global organization with regional teams, different IT systems, and different operational procedures. The people on this team could be call center personnel, onsite technicians, service SMEs (e.g. technical experts), or managers who are looking at the business side of things. This team defines what should be done with those alarms when they get them, and they define what they need from the other teams so they can take effective and prompt action. The Service team also monitors the alarm quality in the short and long term to ensure they are not wasting time and money on bad alarms.\nGoverning Team: This team is a necessary evil to ensure the teams above are aligned and all working towards the same goal. I say necessary evil because a Governing team will slow things down in the short-term and make it harder to take fast decisions. At the same time, a good Governing team will help ensure the project doesn’t go in the wrong direction for eighteen months and then get shut down. With so many cross-functional teams, it’s almost inevitable people will focus on their own goals, even if those goals are contradictory to the overall project being successful. This team defines standards and procedures to follow, and they enforce a quality bar ensuring everybody knows what the targets are.\nUnfortunately, you can also have a bad Governing team which creates misery in the short-term and failure in the longer-term. So having a Governing team is not automatically a solution to cross-functional drama, and they can make things worse, resulting in a lot of passive-aggressive behavior and the inevitable failure of the project.\nHidden Teams: There are also other teams involved in this process that aren’t always at the forefront, but they are critical to it’s success. An obvious example are the people who maintain all the IT systems (e.g. Databases, ERP/Ticketing, Dashboard/BI). They are the ones who built these systems in the first place, and they are the ones who fix issues and implement changes. You also have Data Engineering teams, who build pipelines to ingest raw data and make is useable to other teams. Software Vendors also hide behind all of this. A documentation team may also be needed to take what is created by these individual teams and expose it to the wider world. And if you are selling predictive maintenance as a service, don’t forget sales and marketing.\nHow These People and Teams Work Together SMEs and the Data Team: Typically the people who spend the most time working together are SMEs and Data Scientists. This is because these are the ones who conceptualize and build a solution. At a bare minimum, SMEs have the domain knowledge to identify, support, and justify a predictive maintenance use case. And a Data Scientist has the machine learning and software skills to figure out how to, and if it’s possible, to build a solution they can apply to a fleet of machines.\nHere’s a example of how they work together. An SME is tasked with reducing unplanned downtime on customer machines. They look at the historical service data and identify the top 30 issues leading to the most downtime. They combine this with their domain (e.g. knowledge of the the machine hardware) expertise to filter this list down to issues where it makes sense to try and proactively address them, and they suspect could be predicted using a model. Based on some further business and financial criteria, these use cases are prioritized.\nThis prioritized set of use cases is typically where the Data Scientist starts. They work directly with the SMEs, who provide all the knowledge about the use case, the machine, and the parts/components and how they are related to the failure to be predicted. This can include information about the software logic controlling the machine and generating the machine data. Working together, they also find examples in the historical data of what is supposed to predicted. Using this information, the data person tries to build a model that can be applied to every machine in the field. It’s also the Data Scientist’s responsibility to evaluate the predictive quality of their models, and validate these quality results with the SMEs.\nIn my experience, what I’ve described in the last two paragraphs takes up the bulk of the time with any predictive maintenance project. Unfortunately, this is also the most unpredictable from a time perspective. I’ve been involved in projects which went from first conversation to production in 5 weeks, then a similar use case on a different part of the machine went in circles for months. This back-and-forth between the SMEs and Data Scientist is fundamentally applied research, not engineering. Some unpredictability is expected, and I think the best way to deal with it is to set time limits. After a predefined amount of time (e.g. three weeks), you move to a different use case or provide a very solid justification of why you need to keep working on the current one.\nDevelopment Team and Deployment Team Once the SME and Data person agree they have a good model, it needs to be deployed. There are different ways to do this. At some companies the model development and deployment team are distinct, and the development team hands it off to the deployment team, who puts it into production. In others, the developers take their development code and make any needed changes so it can be moved to production.\nHowever people choose to do it, there should be a defined process stating guidelines, requirements, and expectations on a software/code and business level (e.g. what needs to be provided to the documentation team, any required approvals, etc.). Moving the code to production should take a lot less people hours than the development of the model.\nData Team and the Alarm/Service Team: This relationship is actually the the most important one for the Data team’s success. alarms are useless unless somebody effectively acts on them. Even if a model generates perfect alarms, the value is zero (or negative!) unless somebody does something with them. On the other side, if the predictive quality of the alarms are mediocre, the Service team will lose faith in them and ignore them. While it’s easy to think the SME team is the customer because they propose the use cases and approve the models, the customer is actually the Service team because they use what has been built. If your customer is unhappy, they won’t act on the alarms. If that happens, the Data team has failed.\nThe working relationship between these two teams is oriented around a defined process of what service needs to successfully act on an alarm. For example, they might say every model/alarm needs to come with a predefined set of diagnostic and action steps a service technician needs to do. This could also include educational training so the service personnel understands the purpose of an alarm and what to do with the predefined action steps. It’s then the responsibility of the Data team, SME team, and whomever else, to make sure these deliverables are created before a model goes into production. If the Data team wants to succeed, it’s their responsibility to ensure the process steps, deliverables, and information given to the Service team enable them to successfully act on the alarms.\nThe technical and business processes between these two teams should be defined upfront and should be revisited every 3-6 months. This should include a quality review of the alarms from the perspective of the Service team. Waiting longer than 3-6 months means any flaws in this process risk becoming festering wounds. So it’s important to bring these issues to light quickly.\nWhen things are going well, there should be far less interaction between the Data and Service team when compared to the Data and SME team. The “normal” working interaction should be following the existing process steps as the Data team moves something to production and the Service personnel take over. Divergences from this should typically only occur if there is an issue with the process and the teams are working together to fix it.\nGoverning Team and Everybody Else: In theory, the Governing team has two broad goals.\nDefining how all these teams are going to work together. Defining guidelines and processes so the overall project can succeed. To perform those two points successfully, there should be success criteria, such as quality metrics for models/alarms and what quality level is acceptable for a production model. They should also define things like the minimum requirements for the financial and business viability of use cases. The Governing team should meet once a quarter to discuss all these topics, and have an open forum so people can voice process concerns. This feedback should be used to fix issues to ensure projects are successful.\nA Governing team should also be able to recognize, or at least acknowledge, the size and importance of a problem and the time and organizational investment required to address it. It’s easy to see small problems as big ones due to a loud voice, or big problems as small ones because nobody understands the scope of it. An example of this is the Governing body asking a cross-functional team to define a quality metric to decide when a model is acceptable for production. This seems like a small task, but with different incentives, politics, and different perceptions about data quality across organizational boundaries, this seemingly small task can turn into prolonged war. It’s important for the Governing body to be able to step in and recognize this is an important problem they need to help solve, and not just treat it as a small ignorable side-effect of corporate politics.\nIn my view, the Governing team should meet no more than once a month to review the current working process and to allow people to formally raise concerns. If there is an issue requiring further work, a working group (who meets more often in the short-term) can be formed to address it. If the Governance team is meeting very often (e.g. every week) for an extended period, your process is broken. The whole point is for them to come up with a process for the different teams to effectively work together without constant intervention. If the Governance team members need to meet with the other teams every week to deal with process issues, it means your process isn’t working and the process for coming up with your governing framework isn’t working either. It’s okay to have a two week recurring meeting when you first start a predictive maintenance program and you need to make sure the framework is right. But if you are still meeting every two weeks a year later, maybe your governance team needs their own governance team.\nHidden teams and Everybody Else: In my experience, the interaction between the Hidden and Data teams is very task oriented. For example, if there is an authentication issue on a cloud cluster, a ticket is filed and somebody from the Cloud team supports this. Once that issue is resolved, the interaction on the task stops. The duration of these interactions depends on the size of the issue. For example, an issue requiring a longer fix is when the documentation team needs to update the action plan for every single model deployed in the last three years. This interaction will last longer than smaller issues, but it still stops once the task is complete. In the grand scheme of things, the interaction with these Hidden teams is a small fraction of the total interaction with the other teams.\nPricing Let’s now go back to how I started this post. To accurately estimate how much a predictive maintenance program costs, you should take the following things into account.\nThe full or fractional salaries of SMEs (maybe teams of SMEs) Data People Service Team Other teams needed to support the effort To calculate the fractional salaries, you’ll need to account for time spent in Meetings internal to each team Meetings between teams on short-term topics (e.g. developing a use case) Alignment meetings and Governing body meetings for longer-term topics Time spent following processes, completing templates, and other annoying things of this nature Support Costs, which include Fixing issues with old models Updating existing models due to data changes Updating infrastructure code to deal with upstream and downstream changes Responding to support tickets (e.g. a broken dashboard or alarm that seems weird) Infrastructure Costs, which is the compute/storage and/or hardware/software costs of running all of this Just to be clear, when I say “estimate the cost”, I don’t literally mean you sit down with a spreadsheet and insert items like “Person X = hourly salary * 2 hours of meetings + …” This is simply an order of magnitude analysis to see if all of this makes sense. If you have a low complexity project setup, your team is two people, and your infrastructure costs are $10/month, then having a use case that saves $500k a year makes sense. For a high complexity project, saving $1 million a year doesn’t make sense. When you have a program with lots of teams and people, even saving $5 million a year might not be worth it.\nBad Success Criteria will Kill Your Program For any predictive maintenance projects to succeed, there needs to be a well defined success criteria so everybody understands the goal. If your success criteria is not clear or is something vague and long term (e.g. the total target cost savings over two years is X million dollars), individual teams are going to interpret this in a way favoring their own metrics, and a lot time is going to be wasted in constant realignment.\nA concrete success criteria will be based on the actual service and alarm data. This will enable you to properly estimate your costs and cost savings for each alarm. It should have points like this. Before Developing a Model\nTo be approved for development, each use case needs to clearly show at least $75k in savings This needs to be shown from a time and materials perspective What is the dollar value of a good alarm (true positive). In other words, when the model correctly predicts an future issue, what is the cost savings of proactively addressing the issue? What is the dollar value of a bad alarm (false positive). In other words, when the model predicted an issue would occur but it did not, what would it cost us (e.g. the Service team) to perform an unnecessary service? What estimated threshold of good to bad alarms is acceptable? For example, if the only way for a use case to financially work is to have 97% good alarms and only 3% bad alarms, I can almost guarantee you will not hit it. Service data tends to be messy, and it’s extremely difficult to reach that without extremely clean or manipulated data. Do all the teams agree these estimates are accurate? After Developing a Model (Proposing it for Production) Back testing over six months of historical data, add up the cost savings from the good alarms and the costs from the bad alarms. Are you seeing the expected savings? Do all the teams agree these estimates are accurate? Notice I’ve included “Do all the teams agree these estimates are accurate?” both before and after the model is developed. This is because different teams can have wildly different interpretations of value, even when they are looking at the same data. I’ve seen cases where a Service team has looked at a use case with a seemingly good cost savings and said “Due to the time required to fix this, we don’t have the personnel to proactively fix this issue. We will only fix this during quarterly maintenance when we take the machine apart anyway.” Insights like this can completely change the value of a use case. It’s important for the teams to actually agree on the value before anything is built.\nA bad governance structure can be a big hinderance in this step. In my experience, Governance team members tend to be managers or people with managerial like roles. This means their jobs force them to focus on KPIs and direction, and less about details of those KPIs. They then delegate the details (e.g. how do we estimate the value of a use case) to sub-teams, and inter-team politics takes over. It’s really important for the Governance team to ensure all teams come up with a measurable success criteria everybody agrees to, and they step in when that criteria needs to be updated.\nI also think it’s really important for an honest data person to be part of defining the success criteria. A data person is the only one who can give you an accurate picture of what can truly be achieved based on what they are really seeing in your data. If you don’t have this type of data person, two or three pilot projects might have to be done so somebody can get the experience needed to provide an informed opinion on the success criteria.\nDoing pilot projects to establish a baseline success criteria might not be appealing from a time and costs perspective, but it will lead to more successful projects in the future, better morale (nobody is motivated to work on projects with bad success criteria), and less wasted time in “alignment.”\nHow Does all of this Apply to a Low Complexity Project? Since a low complexity project has far fewer people overall and maybe only a single small cross-functional team, a lot of what is stated above isn’t relevant. What is important is the success criteria. Even if a project only has two people, it’s important for them to agree on a target. This is the only way for both of them to know what they are striving for. If, after meeting four times, the success criteria is “Well we could build a model to detect this, or maybe this other thing, and there’s a third thing maybe” you actually have no success criteria.\nDefining a success criteria doesn’t mean you cannot pivot. Sometimes you spend two months trying to predict a particular issue and realize you cannot predict that issue, but you learned you can build a model for something else. That’s okay and you should pivot. Having a defined success criteria also doesn’t mean you can’t refine it. If you start with “Our predictions need to be 90% accurate” but you realize only 80% is achievable and still a financial win, it’s okay to shift things.\nIt’s also important to recognize while alignment across teams is not an issue when you have one team, agreement with management is still important. SMEs and Data Scientists don’t randomly do projects together just because they can. Typically a manger or somebody else approved the project and reviewed the financial and time costs and goals. So it’s important to at least chat with this manager about your progress, success criteria, and get feedback, especially if you pivot. The last thing you want is your success to become a failure because they expected you to do something different in comparison to the work you actually did.\nMiserable Data Teams This is a tangential topic, but this post is the right place for it. If you talk to people in Data teams (e.g. Data Scientists and Data Analysts), you might meet frustrated people. Online forums are filled with data people who express a lot of frustration at their data jobs, usually tied to misaligned expectations and not getting the support they feel they need. I’m not going to try to establish a single cause for this, but I would like to point out something.\nData teams are ones who see and feel all the dysfunction arising from different teams of people working together when they all have different goals/metrics, and are sometimes incentivized in opposing ways. This is contrast to people who only see the frustrations in their own org, and are somewhat isolated from the chaos around them.\nAn example here might add clarity to this. At one point I had a job in a customer facing role at a company with a lot of different products. For reasons not worth talking about, one of the SaaS products my team supported was struggling in the market. Our entire team was miserable because we spent our days with product issues, frustrated customers, and basically ping-ponging between different internal groups (e.g. sales, product management, engineering management) trying to convince them to change direction because we could see with all the opposing forces and goals of each group, the customer was forgotten.\nOur state of misery was in complete contrast to developers in the Engineering org. We’d complain about an API making no sense to use it the way they expected customers to use it. They’d brag about how the API could handle X million requests per second, how they provided seamless authentication, and how they solved all these interesting technical problems to do this. They had no idea nearly zero customers had been willing to pay for the service after trying it. The developers were buried down at the bottom of a large Engineering org, and to them they were hitting their goals and they were happy.\nMany Data people see problems across organizations. You can be involved in a project spanning three orgs, and realize poor alignment across these orgs means this project is going to fail. But you still have to work on it. And if you do something to make one org happy, the other two are miserable and you are the face of that. The data person becomes the focus point of all this dysfunction because they are only person who understands how all the pieces are supposed to fit together.\nThe flipside of this is Data people can have a unique view of the organization and what is needed to move things in the right direction. Some Data people are exposed to everything from analyzing individual rows of data to looking at organizational level metrics, targets, and motivations. They have what is equivalent to a sociological understanding of what is really going on. They can map how the micro is supposed to feed into the macro, and why things are not working. With the right support, they can help inform leadership about out what’s broken and needs to be fixed.\nWrap Up The process different teams use to work together will define the success of your predictive maintenance efforts. And it’s only when you have a good success criteria that you can define a good process. The best path to the wrong destination is still the wrong path, and if you can’t recognize when you have bad success criteria, you’ll never be able to understand why your process keeps leading to failures.\nI’ve worked on, and been involved in, a lot of predictive maintenance projects, and very few have failed because of a technical decision or the choice to use a simpler algorithm versus a more complex one. If a lot of your projects are not showing any value after they make it to production, or the majority of your projects never make it to production, I’d really recommend you step back and investigate your process and where things are going wrong. Somebody probably knows what the issues are, but what they are trying to say is getting lost in corporate noise.\n","description":"","tags":null,"title":"The Anatomy of a Predictive Maintenance Project","uri":"/posts/the-anatomy-of-a-predictive-maintenance-project/"},{"categories":null,"content":"One mistaken assumption I see people make with predictive maintenance programs is that the cost savings of many lower value projects will add up to a big number, so it’s worth putting in the effort. But sometimes saving a million dollars isn’t worth it if it costs you more than that to implement it. In this post we’ll look at four types of programs and understand why some of them aren’t worth doing because they have a negative return-on-investment (ROI)\nFirst, an anecdote. At one of my previous jobs at a very large company, the CEO announced a contest for people to come up with business proposals on how the company could save money. The CEO tasked SVPs and their teams to come up with ideas and estimate the cost savings of each.\nOur SVP got us together to discuss, and something he said completely shocked me. “If you’re going to propose something requiring a business org outside ours, it needs to save at least a million dollars. If it’s less than that I don’t want to hear about it because we’ll spend more than that to implement it.”\nThis completely blew my mind at the time. Was he really saying this company was so inefficient that it would cost the company at least a million dollars of people-herding to try and save money Was this how large businesses really worked?\nThis post is basically an extension of this anecdote, because anybody with work experience in a large company knows what the SVP said is true. In predictive maintenance, I’ve seen many instances where people assume once things scale, their predictive maintenance programs will be worth it. But friction between different parts of the company means the program never scales, and all you have is a lot of impressive PowerPoint slides with nice numbers, but the numbers never add up in the real world.\nPredictive Maintenance and Failure Modes To make sure we’re all thinking about the same type of project, let’s define the use case(s) this post is referring to. As stated in the title, this is about predictive maintenance. And by predictive maintenance, we are referring to the idea that we want to predict when a machine fails in defined way and mitigate the effects of this failure, also known as a failure mode.\nA failure mode is another way of saying a specific type of failure. For example, if a plastic hose cracks and leaks, the hose cracking is specific type of failure mode. Note we can only predict certain types of failure modes. If somebody accidently drops their phone into lava and the phone melts, the melting due to high heat is also a failure mode. But that’s a very atypical failure mode, and not something we are going to expect. A simple machine (e.g. an automated can opener) might only have a limited number of failure modes. A more complex machine might have thousands of parts and thousands of failure modes.\nThe cost of failure mode is highly dependent on context. A leaking pipe might leak a bit of water onto the floor and not cause any short term machine issues at all. Or if it leaks onto a circuit board and causes an electrical failure, the entire machine can stop working and that is a very high impact failure mode. This context gives us the information we need to define the value and severity of each failure mode.\nTypically, but not always, predictive maintenance models are focused on predicting or detecting known failure modes. The reason for this is if a model predicts a known failure mode, a subject matter expert knows what to look at to validate the issue and what to do to fix it. This greatly accelerates the mitigation or resolution of the impact of that failure mode occurring.\nAt least in my experience, the more general purpose use cases providing a generic “something is wrong with the machine” aren’t very useful from a business perspective. They are impressive in slides (AI based anomaly detection!!), but it is difficult to take any sort of concrete action based on that information. Imagine if your car had a “something is wrong with your car” notification, but no additional information beyond that. Would that be useful to you? Would you pay extra for that “Smart AI” feature? Probably not!\nThe Predictive Maintenance 4-Box Now that we’ve defined failure mode and its relevance to predictive maintenance, let’s classify these failure modes on two axes. First, a specific failure mode can be high or low business value. Business value is defined as Return On Investment, which means however you measure it (cost savings, increased sales, profits, etc), that is how much more value you generate than it cost to do it. Just for concreteness, let’s say a Low Business Value failure mode is $50K US Dollars or less in ROI. A High Business Value failure mode is $150K US Dollars or more in ROI. These numbers may be smaller or bigger depending on the company and use cases so don’t take them as set in stone.\nA business can choose to focus on anything in the range of a handful of high value failure modes to a lot of lower value failure modes, or even a mix of both. Where they focus their efforts will determine how much of a ROI they can realize.\nTo create a models for these failure modes, the effort required exists on a spectrum of needing a lot of manual work with a lot of customization, or they can be fully automated with little human intervention is required. Most projects will probably be in the middle, requiring a mix of both. I’m going to talk more about how this impacts ROI soon.\nLet’s plot all of this in a 4-box. This chart might seem extremely obvious to many readers, but you’d be surprised how many teams and projects end up in the bottom half of this chart while assuming they are in the top half.\n%%{init: {\"themeVariables\": {\"quadrant3Fill\": \"#fe8487\"} }}%% quadrantChart title Is your Predictive Maintenance Program Worth Doing? x-axis Manual Model Development --\u003e Automated Model Development y-axis Low Business Value --\u003e High Business Value quadrant-1 $$$ Profit $$$ quadrant-2 Might be worth it quadrant-3 Don't Do it for the money quadrant-4 Might be worth it It’s important to recognize this chart is summarizing an entire predictive maintenance program, and not just a single failure mode. If you have twenty use cases and each of them has an ROI of $75K, the total ROI of your entire program is $1.5 million. On the other hand, if you have one use case with an ROI of $250K and five others, where each turns out to have an ROI of negative $50K, maybe it’s not worth it even to do the $250K one because it’ll cost you more than that to implement.\nI’m going to talk about each quadrant in more detail, but let’s take a tangent first. I think the challenges with automating things can be greatly underestimated, and it’s one of the major ways I’ve seen programs slowly slide into a negative ROI. So it’s worth it to clarify what automating model development entails, and where the challenges are.\nHow much Effort is needed to Develop a Model? Manual and Automated Model Development is really about the people hours needed to build a model. A model might be anything from a simple set of rules to something complex with deep learning. However, model building is not just about the actual model. To build a good model, you need the right data and right context. And once the model is built, you need to integrate with other systems (e.g. a repair ticketing system) so customer facing people (like machine technicians) can use the model predictions/outputs to make decisions and do something.\nTo understand what is meant by manual vs automated model development, it’s useful to have a high level overview of the steps to build a useful model and deploy it into a production setting. In the list below I’ve bolded the steps that can resist all forms of automation and be so time consuming there’s simply no way to have a positive ROI.\nCorrectly identifying the problem that needs to be solved and defining a solution with a positive ROI Data work Data curation (finding the right data that identifies the problem) Data cleaning and preparation Building a data pipeline Building a model Deploying the model and generating output/predictions Integrating with external systems Training and working with the people who perform actions based on the model output The first point might surprise people, but I’ve frequently seen situations where people can describe a high level problem with high ROI potential, but once it’s broken down into a tractable problem it loses most of the ROI.\nFor example, let’s say the problem is “low fluid pressure.” When trying to turn this problem into something that can be solved using data and a model, it turns out there are multiple causes and fixes ranging from changing a leaky pipe (easy quick fix) to replacing a major component of the fluid system (a harder fix that takes much more time). What was a single problem with a previously idealized single solution has now turned into multiple solutions. Multiple solutions may require multiple models and those require all the people-hours required to build them. There is also the possibility some of those models don’t work out because of poor performance, resulting in a further decrease of the overall ROI, or even pushing the use case into a negative ROI.\nThe second point (about Data) in the list is really about ensuring a clear mapping between the datapoints being collected and the problem. First, you need to be able to clearly identify which datapoints (out of all data being collected) provide the signal needed to detect the problem. Second, you need to be able to clearly identify when the issue is occurring (abnormal operation) and when it’s not occurring (normal operation). This data mapping can be very difficult to scale, meaning for every problem you are trying to capture and predict, the data mapping has to be done manually and with a lot of back-and-forth with different SMEs.\nThe last point is all about the human processes downstream of the output of a model. A prediction from an model indicating an issue might occur isn’t useful in itself. There has to be a process for that prediction to be turned into a concrete set of actions people do to diagnose and fix (if needed) the machine. That means all the downstream systems need to be able to process the model output, an action plan based on that output needs to be well defined, and the personnel acting on that plan need to be correctly trained to perform it. Any flaw in this process can result in incorrect or unnecessary actions or misdiagnosis, driving up costs instead of reducing them. If every action plan for every model has to go through an iterative process to create and get it right, that creates more manual work which drives up costs.\nAutomation And Scaling The primary value of automation comes from being able to increase your ROI by scaling what you have built. There are two ways of scaling relevant to this post. The first is having a model with enough flexibility to deploy across a fleet of machines. This type of scaling works well because you can achieve a high ROI when you add up the total cost savings from each machine. One of the benefits of this type of scaling is since you are focused on one model/failure mode, it’s easier to fail fast and recognize when a model can’t be built. For whatever reason, there seems to be less ego in acknowledging failure here since you are only letting go of a single use case.\nThe second way to scale is building models for multiple failure modes. With this, the value comes from being able to add up the cost savings from the solution to each failure mode. Even if the ROI for each individual solution is lower, the total ROI is higher if the models can be built and deployed without a lot of manual work. In my experience this is where, unfortunately, a lot of people misjudge how much they can automate the development of each solution for each failure mode. And since the plan was for multiple failure modes to combine into acceptable ROI, it takes much longer for people to recognize and admit the numbers just don’t add up. I’ll talk more about this in the next section.\nExploring the Quadrants Now that we’ve defined the axes terms and we have an understanding of scaling and how manual work can drive up costs, let’s discuss the different quadrants.\nHigh Business Value and Automatable In this quadrant a typical use case is a high value fault mode occurring often enough that building an automated solution lets you scale the cost savings across machines. The “automated” part of this doesn’t have to be the development of the initial model, but it’s more about building variations of the model (e.g. for machines running in different environmental conditions), model retraining, etc.\nAn example is seen with oil and gas equipment deep underground. Simply accessing the machine underground, let alone repairing or replacing it, can cost a lot. Let’s say there is an issue resulting in a broken machine, which costs $150k to repair/replace, which includes lost production costs due to downtime. You have 500 machines worldwide, and in the past you’ve seen an average of 1 of these fault modes a year per machine. An intervention can be done to mitigate this, but the complex set of conditions causing the fault need to be accurately detected for the mitigation to work. Since the machines are running in different parts of the world, the environmental (e.g. the ground is mostly sand versus a lot of mud) conditions are different and a different model is required for each set of operating conditions.\nIt doesn’t take a lot of math to see even if an early detection model only accurately catches 50% of future machine failures, that still leaves a large margin to invest in building and operating a solution with a high ROI. And if more machines are added, the cost savings automatically scales.\nOne question people could ask about the quadrant is “I have ten failure modes that theoretically add up to a high ROI. Would that fit into this quadrant?” This question can only be answered after you start implementing the solution(s). If you implement the first two manually, and then use that knowledge to automate the implementation of the next 8, and if your ROI numbers turn out to be accurate, then yes, you are in this quadrant. The risk with 10 different failure modes is that there are many opportunities where your ROI expectations are incorrect or the automation simply doesn’t work. With ten failure modes it will take you a lot longer to determine if you can build the solution(s), as compared to a single high ROI fault mode where you are likely to figure that out much faster.\nHigh Business Value and Manual Model Development (Resists Automation) There is a type of problem that is very high value, but developing an automated solution is difficult, resulting in no opportunity to scale. This can happen because you have a very small fleet of specialized machines and there’s limited opportunity to multiply the cost savings of each machine. This can also happen if the data is very dirty and requires a lot of human provided context to clean, providing limited ways to automate and share this cleaning process across use cases.\nThis can also occur from a low sample size of your problem. If there is an complex intermix of events causing your whole factory to shutdown for a week, but it’s happened three times in the last year, it’s hard to have enough data to separate the abnormal from normal operations. You recognize solving this problem can save a lot of money, but the cost of solving it is completely unknown.\nYou can also be in this quadrant when the ROI shouldn’t be measured purely from a financial perspective. It could be a product safety issue resulting in injuries or death. Or it could be something that destroys the reputation of the company. Some programs in this quadrant are done to appease regulators or because the company got in trouble in the past.\nEven if a highly automated solution cannot be built, a possible alternative is a mix of human decision making being informed by models. A single or set of model(s) can provide insight into where to look, and pre-built data analysis tools (e.g. a highly interactive dashboard) provide the mechanism to how to look. Data Scientists/Analysts and Subject Matter Experts use these to unravel what is going on. That mix of brain and machine are what it takes to catch the problem before it happens.\nThis just speculation on my part, but when I’ve seen business pursue this type of problem, the cost savings, or potential cost savings, is well over $1 Million. In other words, the problem is worth enough that you’re willing to tolerate a complex mix of pre-built tooling (the automation) and also allocating a team of people to investigate manually.\nLow Business Value and Automatable In this quadrant, and also the bottom half of the square, you’ve identified a number of lower value fault modes, when added together have a positive ROI. In other words, a handful of these use cases on their own aren’t worth it, but a lot of them together make sense. This quadrant could also be called the big box retailer strategy, which is to make a small profit on each item sold and rely on high sales volume to add those small bits together to make a big profit.\nThe key to succeeding here is automating the model building process so the solution for each failure mode can be built quickly. To do this, there are two key things that have to be in place\nWell curated data with good labels, which clearly indicates when the failure mode has occurred and when it has not occurred. A fast and repeatable process for translating SME knowledge into a solution (e.g. Data Engineering and Model code). This is essential for mapping the failure mode to the right data that identifies and precedes the failure. If your data is well labeled and mapped, you can build ways to automate the data engineering for the training data. This means you can quickly identify the right data for all the failure modes you are building models for, and not get bogged down in a lot of manual data cleaning work along with excessive back and forth with SMEs. If your data is really bad, having the best SMEs in the world isn’t going to save you. A clear sign that you are in the next quadrant (you cannot automate development), is if you have spent six months working on solution(s) and you are still bogged down in a lot of manual data work to build the training data for each use case.\nRegarding the second point in the list above, it’s also common to be in a situation where once you start working on a use case, it becomes clear there is a lot of additional context needed nobody initially realized was important. It can take weeks or months of discussions to uncover all this context because it takes that long to discover all the right questions to ask. It’s expected for the first few use cases you implement, the process is going to take more time and have more friction. Initially you have to learn how to streamline the process of defining use cases, the communication strategy with SMEs/stakeholders, and identifying the right data. But if after the tenth use case this process is almost as manual and cumbersome as the first four, you should make sure the process can really be streamlined or if you’ll never be able to automate it.\nSomething I’ve seen happen is the technology parts of these processes get automated, but the human parts never get streamlined beyond people building more trust with each other and creating some PowerPoint/Excel templates for defining the use case. The end result is the infrastructure to develop and run these models in production and the integration with the service ticketing system is ready to go. But the parts where the use case and “win” is clearly defined, the data is clearly identified and well labeled, and service personal are trained well enough to handle the alarms, is never streamlined. What you end up with is a system ready to scale, but it never scales because you cannot automate the hard parts. From there you fall directly into the next quadrant.\nLow Business Value and Manual Model Development (Resists Automation) This is the quadrant I’m going to write the most about because I’ve seen many companies end up here and not really recognize (or admit) it, even if some individuals involved in the project realize they are here. This quadrant is also not limited to predictive maintenance or industrial machines. People in other industries will recognize it as well.\nIn this quadrant, you are unable to scale the development of your lower ROI use cases because of human and data issues. You never achieve your projected total ROI because you spend too much time and money developing solutions for each failure mode. In the end, your ROI is actually negative because you spend more money building models and dealing with issues than anything you actually save.\nLet’s look at some numbers. Assume each of your use cases generates a projected $50K cost savings. You’ve been working on these for a year with a team of two data scientists and one data infrastructure person. It takes about two months for each use case to move into production.\nAfter the first year, you get ten use cases into production, meaning you recognize $500K in cost savings on some spreadsheet. Keep in mind it will take some time to see the actual cost savings materialize in the field, so this $500K is really just a forward looking estimate. This cost savings isn’t even going to pay the salaries/benefits of the three team members, let alone all the other infrastructure and support people needed to realize that cost savings.\nWhen you started the project you expected for the first two years the ROI will be negative, but after that the math will work out. In year two you complete ten more use cases. In year three, ten more. Now you are saving $1.5M, which looks a lot better. Plus, over three years, the number of machines out in the field is projected to increase, so this savings will be even higher.\nHowever, there are different reasons these cost savings numbers can be misleading. Let’s look at a few.\nLet’s talk about true and false positives. A true positive is when your model correctly predicts a failure mode is going to occur. You realize a cost savings when somebody acts on that prediction and reduces or removes the impact of that failure mode. A false positive is when this prediction is incorrect and that failure mode is not actually going to happen. Every false positive means a person spends time and money diagnosing or trying to fix a problem that doesn’t exist. The number of false positives scales with the number of machines in the field. More machines means more false positives, which means more costs. Every false positive costs money, and if the ROI wasn’t very high to begin with, the ROI becomes negative very quickly.\nAnother issue with false positives is at a certain point, dealing with them makes people distrust the entire system. If people don’t trust the Alarms, they won’t even trust or act on the true positives. If this happens, all the potential savings goes to zero.\nHow bad the “rate” of false positives feels is also affected by perception. If you have one false alarm out of ten alarms, it doesn’t feel bad. If you have ten false alarms out of hundred, or hundred out of a thousand, the percentage of false alarms is the same in all cases. But if you are the person acting on the alarms and dealing with the pain of checking ten or a hundred bad Alarms, your perception of the model is bad. So it’s not just about the percentage of bad alarms, but really how much impact these are having on your field personnel.\nAnother factor that can decrease your ROI is predictive accuracy degrading over time with any hardware (e.g. a frequently failing part is reengineered) or software changes with machines. The more models you have, the more effort is required to monitor them and make sure they are performing well. Perhaps your Data Science team targets to release ten new models a year and you do this the first year. But in the second year, your development slows down because you need to ensure those ten models continue to run well. The effort to review and maintain older models only increases over time as newly released models inevitably become old models. All of this will either slow down the rate of which you can address new use cases or you have to grow the team, which will increase the costs.\nThere is also an opportunity cost to a predictive service program. Beyond the core development team, there’s the time investment of all the other people (e.g. SMEs, Data Engineers, Service Engineers) to support this. They could have spent their time doing something else, so it’s important to account for their costs when justifying your program.\nAll of these issues (and many others) mean your original ROI estimates are very optimistic. They might assume a perfect system that identifies every issue without errors, and every issue is dealt with perfectly by the service personnel. That $1.5M estimate for the third year can get cut quite drastically and you might not even get a third of it. If you are a billion dollar business, is it really worth investing in a project providing $500K annual cost savings after three years? You could fire your Data Science team and save that money with much less effort.\nHow do people end up in this quadrant? What I’ve seen happen is a predictive service program being conceptualized with some number large (e.g. $10M/year) enough for upper management to take it seriously. Then, as the program is implemented, the actual savings is not well tracked and people refer to the original financial projections when asked. As employees and managers change, teams reorg, and priorities and ownership change, the line from that original number to today becomes even more blurry. After two years, nobody can clearly state where that original number came from and how it ties to the current state of things. But it’s still a target, so people stick their head in the sand and try to hit it.\nCompanies can also land in this quadrant when they need a predictive service program for strategic reasons. If your competition claims to do “predictive service” it’s going to be challenging for you to tell customers you don’t have it or you don’t believe in the value of it. Sensible strategy turns into dysfunction when the only way to justify this type of program is by making claims of a large financial advantage. I’ve seen this at larger companies, where the only way to get multiple departments (e.g. machine engineering, data science, customer service) to work together is to get buy-in from upper management and the top of the middle-management layer. A bold financial benefit claim is made and set as a target, resulting in a program which is doomed from the start.\nIf you are in this quadrant, getting out is tough. Simply dissolving or watering down the program and rebooting it a few years later with newer people and technology seems to happen often. As much as I dislike the nastiness that comes with doing this, if something is a big enough mess, then destroying and rebuilding it might be easier than trying to fix it.\nIf you do want to climb out of this quadrant, I’d recommend properly identifying the root cause(s) of the struggling program instead of helicoptering in new people to fix it and giving them deadlines not grounded in reality. They’re only going to start covering up the symptoms instead of understanding the root cause.\nWhat are the root causes of this type of program going off the rails? They can be quite varied, including\nDifferent stakeholders having different interpretations of how bad the data quality issues are and what the real impact is on reducing the ROI. Stakeholders having inconsistent definitions of business value, meaning one stakeholder thinks a problem is worth $100K, whereas another stakeholder feels this problem has negative ROI. Too much of a top-down approach, where the most “valuable” use cases that have to be implemented (nobody can say no) are defined in a very academic way, without a robust data analysis of actual field data to support those ROI values. Overly focusing on checking as many “we got it into production” boxes as possible, resulting in high ROI in a spreadsheet, but not in reality. Minimal overlap between use cases or fault modes, making it difficult to share knowledge or share development methodology/code, which reduces opportunities for automation and scaling. Every solution then becomes a unique snowflake. Notice all the points above are really about alignment between the different people involved in the program. Even the fourth point is really about making sure people understand use cases and the associated context need overlap to be scalable. A Data Scientist might understand, but people who are not data professionals might not realize the extent to which overlaps are needed to support automation in a data and software process. If different stakeholders aren’t aligned, even declared successes by one group can be perceived as failures by a different group.\nI’ll finish up this post by talking about what gave me the idea to write it. I’ve seen too many instances where a program failed and people pointed their finger at the symptoms instead of the cause. It’s easy to scapegoat the data science team, but they’re usually only the face of deeper organizational problems. I hope this post provides insight into some of the broader factors that drive success and failure in predictive maintenance programs, and you use them to better triage and diagnose any issues you are having. If nothing else, you know more about what types of projects to avoid.\n","description":"","tags":null,"title":"Is Your Predictive Maintenance Data Science Program Worth it?","uri":"/posts/is-your-predictive-maintenance-data-science-program-worth-it/"},{"categories":null,"content":"In my previous post, we looked at how stakeholders and organizational structures influence decisions about what data is collected and exposed for use. If you haven’t seen machine data before, that post might have felt a little abstract and some things might have felt contradictory (e.g. how does one look at the events before a failure without any sensor data?). For people who prefer concrete details and real examples, this post is for you.\nWhen I was involved in my first predictive maintenance project, I had a fairly naive view of machines and the business of deriving value from machine data. In my mind there was a machine that generated a lot of data and you analyzed that data. I didn’t really recognize or think about topics like\nWhat’s the bare minimum data actually needed to run this machine? What data is not needed, strictly speaking, to run the machine, but is necessary for better diagnostics? What additional data is required for predictive maintenance? What is the cost and effort required to get more data from the machine? Does the cost of a predictive model even make sense? Will the model cost more to build and operate than it saves the company? A good starting place is to understand how a machine works and what data can be expected from it. If we start from the basics, we can gradually build an understanding of how people define the requirements for what data should be collected, and how those requirements change as needs become more complex. This then connects to the previous post, where we see how different stakeholders influence this process.\nA Basic Machine and the Data it Generates The Data Needed to Run a Basic Machine To explore what data is needed to run a basic machine, let’s start with a very simple machine that cleans dirty beakers. The Clean Beakers machine is basically just a conveyor belt that holds beakers and move them under some cleaning nozzles that spray cleaning fluids. Let’s imagine this machine operates in factory that mixes chemicals in glass beakers, and the beakers are reused. The overall process of the factory can be seen in the diagram below.\nflowchart TB LB[Load Beakers] --\u003e CB[Clean Beakers] --\u003e MC[Mix Chemical in Beaker] --\u003e EB[Pour Chemical Out of Beakers] --\u003e LB style CB fill:#e69138 A person starts this process by loading beakers onto the conveyor belt. The beakers are first cleaned, and then moved to another machine that mixes some chemicals in these beakers. The mixed chemical is poured out, and then a person carries these dirty beakers back to the starting point. This process could also be automated. So instead of people, a big conveyor belt moves the beakers between the four stations (each box in the diagram is a station) in this loop.\nNow that we understand how this beaker cleaning machine operates in the larger factory process, let’s look at how the beakers are actually cleaned.\nflowchart TD BP[Beaker Arrives] --\u003e BS[Beaker Washed With Soap] --\u003e BW[Beaker Washed With Water] --\u003e BC[Beaker Clean] An upside down beaker (the mouth of the beaker is facing the floor) enters the machine on the conveyor belt. The conveyor belt takes the beaker and moves it over the soap nozzle. Once the soap is sprayed, the conveyor moves the beaker over the water nozzle and water is sprayed to rinse out the soap. The beaker is now clean and is moved on to the next machine where the beaker is flipped over and the chemicals are mixed.\nWhen this machine was being built and the requirements for what it should do were gathered, the engineering team(s) made decisions about what data this machine needed to run, and what data would be exposed to the outside world. Typically, the more basic the machine, the more basic the data requirements might be.\nIn the most minimal sense, what information or data would be needed for this cleaning machine to function? Let’s first define the way the machine works.\nThe machine consists of a loading area, a single soaping area, and a single water rinse area. There is a simple weight switch to detect if a beaker is present. No weight means no beaker is present, and some weight means a beaker is there. The switch doesn’t actually measure the actual weight, it’s just an on/off or yes beaker/no beaker. Below the soap and wash areas, there is a nozzle. Each nozzle is connected to a valve that controls the water or soap being sprayed. The valve is either on or off, there is no in-between settings. There are a total of 3 switches. One switch where the beaker is loaded on the conveyor belt to detect when something is placed on the belt. One to detect when a beaker is above the soap nozzle. One to detect a beaker for the water nozzle. A simple motor moves the conveyor belt one step (e.g. from above the water nozzle to above the soap nozzle) at a time. Each step is of equal length. A very basic industrial computer (e.g. a PLC) that controls the machine based on inputs from the switches. The computer also controls the valves and the motor. Based on that, this is the logic of how the machine functions and how the computer controls it. The square boxes are actions performed by the machine/computer. The ovals and floating text just provide more information on what is going on or the decision.\nflowchart TB BP([Person Places Beaker On Conveyor]) --\u003e LAS[Check Switch in Loading Area] SS[Check Soap Nozzle Switch] SN[Spray Soap] WS[Check Wash Nozzle Switch] WN[Spray Water] LAS --\u003e |No Beaker| DN[Do Nothing] LAS --\u003e |Beaker Present| MC[Move Conveyor One Step] MC --\u003e SS SS --\u003e |No Beaker| DN2[Do Nothing] SS --\u003e |Beaker Present| SN --\u003e MC2[Move Conveyor One Step] MC2 --\u003e WS WS --\u003e |No Beaker| DN3[Do Nothing] WS --\u003e |Beaker Present| WN --\u003e MC3[Move Conveyor One Step] MC3 --\u003e BCF([Beaker Cleaning Finished]) --\u003e |Repeat Process| BP Another way to read this chart is to see what signals the computer receives and the decision made based on that signal. For example, let’s think about the “Check Switch in Loading Area” box. If somebody places a beaker there, the switch gets triggered and the “on” signal is sent to the computer. The computer then sends a signal to the conveyor motor to move it. Instead, if that switch was “off” because no beaker was there, the computer would do nothing. So the data sent to and from the computer only consists of different on and off signals.\nIf we were to record every event this machine does in order of what is done, it would look like this. This type of data is called Event data and is typically stored in an Event Log.\nEvent Time (hour:minute.second AM/PM) Switch On: Beaker Placed on Machine 1:00.00 PM Conveyor Moved 1:00.30 PM Switch On: Beaker Over Soap Nozzle 1:01.00 PM Soap Nozzle Activated 1:01.30 PM Conveyor Moved 1:02.30 PM Switch On: Beaker Over Water Nozzle 1:03.00 PM Water Nozzle Activated 1:03.30 PM Conveyor Moved 1:04.30 PM The steps above would be repeated over and over in the Event Log.\nThe computer might have some additional logic to make sure a switch is turned off after it’s been turned on and other logic to handle possible errors. But this is the basic logic required for this machine to run, and you can see the data we would get.\nKeep in mind that this data might not be available to anybody to take off the machine. The data you see above could be limited to the computer and no programming or storage was put into place for anybody to actually access this data. So you shouldn’t assume that if you plug your laptop into the machine computer that you’d be able to see this data. Somebody has to write the software code to expose this data to the outside world. But for the sake of making this easier, let’s assume this machine stores this data somewhere and people can access it.\nDiagnosing an Issue Using Data Now that we’ve described this basic machine, let’s move to the scenario of diagnosing an issue using data. One day somebody notices the beakers are not being cleaned properly. Let’s also say that due to the physical design of the machine, it’s hard to just look into it to see why the cleaning is not being done properly. So nobody can just watch it running and see the issue.\nIf somebody pulls 10 days of log data (described in the previous section) to investigate, all of it is going to look exactly the same. There’s no clear indicator of the machine running differently, when the issue started, or what the cause is. The log data tells us the machine is running and we can calculate things like uptime/downtime and utilization (how much this machine is used in a day), but we have very little information about what’s really happening during the various processes the machine performs.\nIf you’re only looking at the data, you’re not going to be able to diagnose the issue, and you’re certainly not going to be able to develop a predictive rule or model about when the machine is going to stop cleaning properly. Despite the fact that the machine is running and providing data, the data isn’t very useful.\nIf we wanted to diagnose the cause of the cleaning issue based purely on data, we could identify possible failure modes and the data needed to detect those. Here are some examples.\nFault Mode Data Required to Diagnose Nozzle Not Fully Opening We need a way to track exactly how much the water and soap nozzle(s) opens Water/Soap Pressure Issue We can track the water pressure in the pipes that go to the nozzles, maybe at different locations along the pipes Conveyor Movement Issues Tracking how much the conveyor actually moves at each step might identify if the beaker is not placed at the right position over the nozzle If we were also collecting this data, maybe we’d see that the water pressure exiting the nozzle has gone down over time. If we have multiple pressure readings along the pipes, maybe we could further diagnose it as a supply issue (not enough water is getting to the machine), a blockage issue (the water supply is fine, but one of the pipes in the machine is getting clogged) or a nozzle pressure issue (the nozzle is not spraying at enough pressure). Based on this, we could build a simple model that Alerts on when the water pressure is trending downwards and is predicted to be too low (to clean the beaker effectively) in the future.\nThis type of data is also known as Sensor data. In contrast to Event data, sensor data gives you information about what was going on during a process, not just after it. Event Data typically tells you about events after the event has already started or occurred, but has limited information about the conditions during the actual process that is occurring during that event. It’s the combination of Event and Sensor data that tells you when a process starts (a Start Event), what is going on during the process (the Sensor Data), and when a process ends (an End Event).\nHow do people decide what data needs to be collected? Imagine when this beaker cleaning machine was designed, the original requirement was to build something priced to sell to small, low-volume factories with limited money. Engineering and Product management decided the way to reduce costs would be to build the minimal product described above.\nOver time, the machine became more popular and higher volume customers started buying it. They also started complaining about it being difficult to service because it was difficult to diagnose issues. These customers also started to make warranty claims out of frustration, stating the machine was malfunctioning due to product defects. And since diagnosis was difficult, it was not clear if this was a genuine warranty claim, a maintenance issue, or people not operating the machine properly.\nTo fix this situation, the service organization proposed a solution. They would create a subscription service plan and fix any machine issues as part of the subscription cost. To do that, they would need the machine to collect the data described in the previous section. They also proposed a “subscription plus” plan which offered remote diagnosis of the problem. That way the problem would be identified quickly before anybody even went onsite, and the service technician would already have the parts needed (along with a repair plan) to fix the machine as quickly and efficiently as possible. To implement the remote diagnostic capability, the machine control computer needed to be upgraded (read: more expensive), the software needed to be updated to expose the data, and a software tool needed to be implemented to transmit data to the remote service center.\nFrom a business standpoint, there are a few considerations with this subscription plan idea.\nThe Service Org needs to provide a business plan demonstrating this subscription service is profitable, and it’s worth the time, effort, and money for Engineering to update the machine Product Management needs to understand if customers are willing to pay for this updated machine and associated service plan. Are there enough potential customers? Engineering needs to look at their priorities and decide if it makes sense to make these updates to the machine, or to have 2 versions of this machine. One basic version, and one with the extra data capabilities. And based on that, can they can support both machines? The Data Infrastructure org also needs to be made aware that this service is coming and they will need to build a pipeline to ingest the machine data, process it, and expose it in a usable way (e.g. Dashboards) to the personnel in the Service Org. The specifications of the pipeline might influence what data is sent from the machine since the Data Engineers might need something specific that’s not in the existing machine data requirements. And whatever this costs to do, it needs to be included in calculations for the service plan. The Marketing and Sales Orgs will probably need to be involved as well since they can help provide insight into the customer demand for these features. Assuming the product updates are made, these orgs will definitely need support with their go to market strategy to market and sell it. Moving to Predictive Analytics Two years have gone by. Product management and Engineering decided on two machines, and the second one not only included the extra data capabilities, but it could clean more bottles per hour. This brought in a lot of new customers who were willing to pay for the plus plan that includes remote diagnosis. Customers really like the new service, and the subscription plan is profitable as well.\nHowever, there are two growing concerns. The first is that the service department only knows about a machine issue when a customer calls and creates a ticket. Basically, the service is 100% reactive and customers are already experiencing an issue by the time they call. Customers are increasingly asking for automatic and proactive identification of machine issues before it affects them.\nThe second is that the remote diagnosis is completely manual. After a customer calls, somebody with diagnostic training opens dashboards, identifies the issue, and then provides a recommend plan to fix it. Since the diagnostic process is manual, the number of technicians has grown in proportion to the number of customers. However, from a budget perspective, this isn’t scalable because the number of diagnosticians required is cutting into the profit from the subscription revenue.\nTo address these two problems, the Data org is asked to come in and evaluate the feasibility of automated detection of machine malfunctions before the if effects the customer, sending out a notification Alarm based on this, and including a recommended fix plan with every Alarm. The Data team spends some time evaluating this and provides the following recommendations\nCurrently, the data from each machine is a daily summary of the sensor values. For example, daily maximum water pressure, daily minimum water pressure, and daily average water pressure. Since it is a daily summary, it is only sent once a day. The Data team feels that for good predictive service, they prefer the raw un-aggregated sensor data in an ideal case, but hourly summaries would be sufficient. It would also need to be sent 6 times a day. They need a budget for the infrastructure to develop and deploy these models The infrastructure will need to integrate with the other systems, such as the Service system that manages customer tickets. They need a dedicated team of at least one Data Engineer and one Data Scientist to be able to support building the models, but also to support any tickets about the data quality, models, and Alarms. All of the above will eat into the revenue pie of the subscription services, so it’s important to estimate how much automating these things will save the company compared to just hiring more people to perform the remote diagnoses. In other words, how much cost reduction is realistic if the Data org is able to successfully implement their ideas? How accurate do the Alarms and recommendations have to be to actually realize these cost savings and for it to be considered a success?\nThe Motivations for Data Collection At this point we can stop building out this story. There are many different directions this could go, all of which would further illustrate the original goal of this post and the previous one; that data is not collected and provided just because that’s what one does. Collecting data takes time, effort, and money, and there needs to be a useful reason to why it’s done.\nStakeholders can be very differently oriented in how they think about the value of data and where investments should be made. Engineering is focused on what data is needed to ensure the machine runs. Service wants data that allows them to service the machines as cheaply and quickly as possible while balancing driving down service costs with keeping customers happy.\nIn the predictive analytics section of this story story, the Data team is really an extension of Service. If the Data team said they wanted higher frequency data just because they want more data, Engineering would completely deprioritize the request. However, since this is really a request to reduce costs and improve what the Service org provides to the customer, the Engineering group might be pushed to prioritize it.\nSometimes these relationships are adversarial. The Engineering org could disagree with the way the Service org wants to do this and they feel the machines can have the predictive algorithms running on the machine computer. That way there’s no need for the Data org to get involved at all, and the Service department doesn’t need to own it. If the code is running on the machine, the slice of the subscription revenue allocated to the predictive service feature will go to Engineering and not the Service Org.\nIf we think back to the previous post where we talked about all of these functions being in the same company versus multiple companies, each one has it’s positives and negatives. For example, if the company that built this machine was purely a machine builder, they might say they don’t want to get into the servicing business. The most they’ll do is some minimal work to make it easier for service providers to add data export and transmission capabilities to the machine. When the machine builder and service are different companies, there’s a clearer delineation on where each should focus. If the Engineering and Service org are in the same company, there might be more push from upper management for the groups to work together, since any cost reduction will benefit the company as a whole. At the same time, this might result in a worse quality predictive service product because both of them have no choice but to work together and neither wants to hold the other accountable for issues and risk harming existing relationships.\nThe Evolution of Industrial Machines and Technology In the example in this post, the machine is very simple and only needs a very basic computer running a relatively simple set of instructions to control the machine. Even if the machine was collecting the hypothetical sensor data, it would still only be a few sensors and an easily manageable set of data.\nIn contrast, there are machines so complex they have hundreds or thousands of sensors, multiple computers, and lots of additional microprocessors to control everything. The software running on these machines is also just as complex, and is basically a product in itself. Examples of these complex machines include the lab equipment used to analyze human blood samples and machines used to manufacture microchips.\nWhen you hear about or see these types of machines, it’s hard to wrap your head around how it was designed and built. One question I always ask myself is how did people even start or plan it out? Imagine if somebody asked you to build a car. Where would you even start? How do you build something so complex from scratch?\nThe answer is, you don’t build it from scratch. The complex machines are based on the lessons learned from less complex machines, which are based on lessons learned from machines that were less complex than those ones. Over time, people make mistakes and learn how to make things better. They also learn what is needed in the real-world and they evolve their products. Lots of new machines have hardware and software in them that was made and used in an older machines.\nAs an example of this, let’s look at the computer that controls these machines. At first, the control systems were basically all hard-wired and it was very difficult to change the logic. Then came the PLC which gave people much more flexibility than hard-wiring the logic to control the machine. Modern PLCs are basically regular computers (with lots of cores) that have all the software and hardware (e.g. connectivity) to operate in an industrial environment. They can also be extended to run docker (e.g. python in a container), a messaging bus, a relational database, you can attach an NPU (for Deep Learning Inference), and do lots of other things.\nWith this machine evolution comes an evolution in what data is collected as well. As machines are designed and used by customers, people realize what additional data needs to be collected in the next version of the software or machine. Over time, as sensors become smaller/cheaper, and programmable microcontrollers become more flexible and powerful, having more data becomes essential to these more complex machines operating properly. Then the discussion changes from “why collect data” to “what’s the right data to collect?” Here are two examples of this.\nA complex machine already had lots of different event and sensor data being used to control the machine, but due to the high data volume, most of it was not stored and thus never exported. In fact, the software and database on the machine weren’t really designed to store and export all that data outside of some temporary debugging modes. To enable any additional data export would require a major software update that didn’t cause issues with any existing data exports. So for any additional data exports, there had to be good rationale for what data was really needed to address any use cases. A factory was collecting data from multiple machines that were all part of the manufacture of a single thing. A 6 month sample of this data (thousands of sensors) was multiple terabytes compressed. There was a very expensive issue that was occurring, and they were trying to better understand the circumstances under which this issue occurred, not build an accurate model to predict the issue. After an extended timespan where multiple internal and external teams of specialized Engineers and Data Scientists looked at this, the conclusion was that despite collecting so much data, they were not collecting the right data. Note that data limitations do not arise only from machine data. Certain use cases require the alignment of machine and non-machine data and this is not always feasible. One widget manufacturer was facing the problem that some widgets would crack during the final stages of the manufacturing process, and there was a theory that it was a combination of manufacturing process and some supplier quality issue. There was some issue with the raw material that resulted in a reaction to some condition in the manufacturing process. All raw material did undergo a quality review when it was delivered, but nothing stood out. In this particular factory, each widget was created individually, but in later stages many widgets were batched together (e.g. heating lots of widgets together in an oven). So there was no way to match or align the sensor or quality data with the individual widgets and match the data to the widgets that cracked.\nWith what I’ve just said, I don’t mean to imply that machines and data collection have reached this point where every machine is generating a lot of data and the challenges are now around data alignment/cleaning and figuring out which is the right data to collect. Many machines collect minimal data that is useful only for some basic KPIs. It also takes time for Industrial customers to upgrade and update their machines. If a factory was built in 1993, it’s almost a given that many of the original machines are still operating 20+ years later. And those machines were were probably designed based on technology older than 1993! Unless there was good reason to upgrade, the computer controlling those machines would still be that old. That’s assuming the computer controlling the machine isn’t a human!\nWrap Up I hope this post gives some concreteness to the idea of a machine and what data it generates. I realize the topic will still feel a bit vague since I’ve given general examples of machine data, but there’s no spreadsheet here with data extracts from machine and non-machine data. My intent was to add clarity to the idea that data isn’t collected for the sake of being collected, and that there are conscious decisions made about what data is collected an exposed. Also, the calculus around these decisions changes as technology improves, and the justification needed to make a decision today might be different than what was needed a decade ago.\n","description":"","tags":null,"title":"Deciding what Data to Collect","uri":"/posts/deciding-what-data-to-collect/"},{"categories":null,"content":"This post will be the first of two that discusses how organizational dynamics, stakeholder incentives, and the goals of the business drive decisions about what data is collected and why. What is discussed is relevant to companies trying to do predictive maintenance on industrial and commercial machines, but it applies to other industries as well.\nIn my experience, when people first start learning about things you can do with data, there tends to be a focus on the last part of the process, where one is trying to use that data as part of a hypothetical business decision. If you are a Data Scientist, this means you focus on using models make an accurate prediction. If you are in BI person, your goal is to learn how to make a dashboard with useful charts. If you are a Data Engineer, you learn how to transform raw data a more user friendly form.\nThere isn’t much focus on why this data was generated and stored in the first place, which is the start of this data process. Data isn’t collected just because we have the technological capabilities to collect it. Data is collected because people made the decision that collecting this data would be useful, and what it meant to be “useful” was translated into technical requirements for what data needed to be collected.\nWhat people mean by “useful” is not static. Different stakeholders and organizations have different goals and can have very different ideas, sometimes opposing, of how the same dataset can be useful to them. And as companies change, and technology evolves, this can also change what people find useful. All of these stakeholders and changes feed into what people want to do with data, and what data needs to be collected to satisfy this.\nThis post will be the first of two that discusses the organizational dynamics I’ve seen with companies trying to do predictive maintenance. The overall goal is to try and provide some insight into how different stakeholders with different business motivations can influence what data is collected and how it’s used. To get there, it’s useful to look first at the architecture of how data might flow in a predictive maintenance project and then make this more concrete using examples of what data is generated and collected on a machine.\nTo avoid the this post turning into a novella, I will break this into 2 parts.\nThis first post will present some data architectures as a framework to explore how different organizational structures can drive why and what data is collected and how it’s used. Deciding What Data to Collect. The second post will provide more concrete examples of what data people might want to collect from a machine, and their motivations for doing so. It’ll be more technical, but I think this will provide a lot of context to the first post regardless of if you care about technical details or not. I don’t think there’s an absolutely correct way order these posts, but I thought it made sense to start with architecture first. If you prefer to jump from here to the second post and then come back to read the rest of this post, that shouldn’t be an issue.\nIntroduction In this post we’ll look at the data architecture(s) and data flow(s) of a typical use case seen with industrial machines. This post is focused on the situation where there are machines generating data in a remote location (e.g. a factory) and there is a person or group of people who need centralized access (e.g. in a corporate office) to this data. An alternate to this is somebody who is at the same location as the machine and is directly collecting and analyzing data straight from the machine, but this post isn’t focused on this latter type of use case.\nOn the surface, this data flow is quite simple. What I am really trying to communicate is the complexity that arises from different organizations owning the data, and different stakeholders having differing incentives and ideas about how to extract value from this data. So try to avoid taking the architecture diagrams at face value, and instead mentally map them to the organizational abstractions they represent.\nSomebody reading this post might have a lot of questions about the technology choices and data products used in this architecture, but that’s not really the point of this post. The architecture is just the framework to help understand how this data is owned by different organizations in any given company. If it helps you to visualize things, pick your tool(s) of choice and fill the boxes with those.\nArchitecture Overview Let’s start with a very simple data architecture of machine data.\nflowchart LR M[Machine] --\u003e C[Raw Data Landing Zone] --\u003e DW[Data Warehouse] --\u003e DS[Data Scientist] What do each of these boxes mean?\nA Machine Generates Data. This data is stored locally in a database and/or files. Machine data might be Event Data, which has data from known events with established events codes (e.g. event code 03040506 means the a robot arm picked something up) Sensor Data Images/Audio/other non-textual data. A software agent on or connected to the Machine uploads this data to a server somewhere in the form of structured (e.g. XML) or unstructured files. These raw file are securely copied to a server we’ll call a Raw Data Landing Zone. The raw files are processed and exposed in a Data Warehouse. An example of this is if data from machines around the world are combined into one table that users can query. Typically some type of automated Data Engineering occurs between the Raw Data Landing Zone and the Data Warehouse The diagram above is a very basic representation of a use case where a machine is generating data and somebody somewhere is using that data. Let’s add a bit of complexity by adding non-machine data sources. “Non-machine” data is data that can be linked to that machine, but is not generated by that machine.\nflowchart LR M[Machine] --\u003e C[Raw Data Landing Zone] --\u003e DW[Data Warehouse] --\u003e DS[Data Scientist] S[Service Data] --\u003e DW Q[Quality Data] --\u003e DW In addition to data from the machines, we also have Service Data. This is data created by service people and technicians who service these machines. This type of data has both structured and unstructured components. Examples of the structured parts of this data might be When somebody opened a ticket due to a machine issue When a service technician started and stopped working on the problem Some keywords organized in hierarchical list describing the problem (e.g. Subsystem B -\u003e Fluid Pump -\u003e Leaking Outlet Pipe -\u003e Replacement) What work was done and how long it took What parts where changed The unstructured parts might be typed up notes documenting every single interaction with the customer, and also a way for the service technician to describe their work beyond what is possible to record in the structured data (e.g. entered using a form). An example of this might be “Met with machine operator who said this is an intermittent issue, but is happening with increased frequency. No issues reported in self-diagnostics after running it 2 times, but issue occurred as we were observing normal operation.” The actual text might not be this clear and there are likely to be multiple languages in the data at a global company. Quality Data. Either before, during, or after the machine process, some measurement of quality is performed. Sometimes this process is done manually by a person, so you only have snapshots of quality. Some machines can also automatically perform quality checks, but these checks may slow down the machine process and it may not be done every single time. It’s also possible the quality check results were recorded on paper and then entered into a computer later, which can lead to quality issues. Note that in the diagram the Service and Quality data are ingested directly into the Data Warehouse. This is because the data is typically entered using a computer and is stored in a local or cloud database. So there may be a way to query the data directly and feed it into the Data Warehouse and have no need to temporarily store them in the Raw Data Landing Zone\nData Ownership Instead of looking at the data flow purely in the sense of how the data flows, let’s modify this to include the “owners” of the data. “Owners” are the business units who define the requirements for what data needs to be collected so they can be use it to run their part of the business. These are the ones who say “I need this metric/KPI, and this is the data I need to calculate that.”\nIn most companies you’ll also see another form of data ownership, which is who owns the data and data pipeline from a technical standpoint. For example, if you query a database and the data has an error, who do you contact to fix that? There’s typically a technical team that is responsible for those types of issues. This post is not about this latter group of technical owners, but really about the business owners.\nA Single Company Owns Everything Let’s assume all the business and technical owners/organizations are at a single company called Company A that designs, makes, and runs the machine(s). Everything is internal and there are no external vendors who own these functions.\nThe “owners” are shown in the chart below. To make this easier to read, the boxes with red borders (Engineering, Service, and Quality) are the business owners. The other teams work with the data and build things (e.g. Dashboards) with it, but ultimately the business owners are the ones using this data to reduce costs or increase profits\nflowchart TD subgraph Engineering M[Machine] end subgraph Service S[Service Data] end subgraph Quality Q[Quality Data] end subgraph IOT[IoT Group] C[Raw Data Landing Zone] end subgraph DT[Data Team] DW[Data Warehouse] end subgraph A[Analytics] DS[Data Scientst] end style Engineering stroke:#ff0000 style Service stroke:#ff0000 style Quality stroke:#ff0000 M --\u003e IOT DW --\u003e A S --\u003e DT Q --\u003e DT C --\u003e DT Important: It’s important to recognize something here about data ownership. Each organization defines their data requirements based on what they need to run their part of the business. This is in contrast to an org like the the Analytics Org. People in the Analytics Org aren’t defining what data needs to be collected, they are using data that was generated because of the requirements defined by another part of the company. In other words, people in the Analytics org are using data that is a byproduct of some business process, they are not defining why that data needs to be collected in the first place. That being said, it is possible that somebody in the Analytics org will specify additional data that needs to be collected to support a business use case. And then the business unit that asked for that use case will justify to somebody else (e.g. Engineering) why they should collect that data. But even in this case, the Analytics org doesn’t “own” that requirement. It’s the business unit that owns it.\nTo make this concept of ownership more concrete, let’s take the Service org as an example. Service is responsible for ensuring the machines are running properly. If the machine is not running properly somebody will contact the service organization to fix the machine. The service org provides these basic functions\nCall center personnel and to interact with the machine operators and other personnel at the customer(s) Remote and onsite troubleshooting Remote (if possible) and onsite fixes Scheduling onsite visits Dispatching technicians to the machine site if an onsite visit is needed Parts inventory and ordering parts From a data requirements perspective, this translates into\nRecording Customer Contact information and machine (e.g. the machine serial number) details Tracking of machine operator/customer interactions with customer support Documenting what repair/service work was done on the machine and who did it What work was performed and date/time when it was done Work time(s) and travel time(s) for each time work was done Parts used or changed Identification of which system or part in the machine was causing the issue Overall tracking of a Service Ticket Open (when the service department was alerted to the issue) and close (when the issue was resolved) date and time. Total time spent on work and travel Total monetary cost of this service ticket to the customer and the service org The list above would be provided to a technical team as the requirements for an application or software service(s) where this data can be collected (e.g. using a web form), viewed, modified, stored, and exposed to people inside and outside the service org.\nNote that the service org might not collect and store Machine Data (e.g. sensor data) as part of their daily operations. However, they may need machine data to understand the behavior of the machines so they can use it to diagnose problems. So instead of storing this data in their own systems, they may get this data from the Data Warehouse.\nOwnership Spread Across Multiple Companies The nature of ownership and incentives can change quite drastically if all of these functions are at different companies. Let’s change the structure now so that different companies or service providers own a different piece of this diagram.\nFirst, let’s assume that the Machine is built by a Machine Builder, which is a company that builds machines that somebody else uses. A basic example of this is something like a commercial water heater. The company that makes these water heaters probably sells millions of them per year. However, while they design and manufacture the water heaters, they aren’t heavily involved in installing or servicing them. Outside of identifying potential product defects that effect a large percentage of customers, they aren’t interested in collecting near real-time data from each heater. Other examples of machines built by Machine Builders include CNC Machines, Electrical Submersible Pumps used in Oil \u0026 Gas, Electric Vehicle Battery Testing Equipment, etc.\nIn the diagram below, each company is a different color. You can contrast this to the single company chart above, where all the business functions are owned by the same company. To make it easier to understand what each company is doing, I’ve modified the text in the boxes to describe what they are doing. The data is a by-product of this (e.g. Servicing machines creates service data).\nflowchart TD subgraph MB [Machine Builder] DSM[Designs and Sells Machine] end subgraph Customer M[Uses Machines] Q[Measures Quality] end subgraph TPS[Third Party Service] S[Services Machines] end subgraph ITSP[IT Service Provider] RDLZ[Collects Raw Data] DT[Performs Data Engineering] DW[CreatesData Warehouse] end subgraph AC[Analytics Company] DS[Builds Analytics] end style MB fill:#9fc5e8 style TPS fill:#ffd966 style Customer fill:#8fd24b style ITSP fill:#e69138 MB -. Sells Machine To .-\u003e Customer Customer --\u003e ITSP S --\u003e ITSP Q --\u003e ITSP ITSP --\u003e AC RDLZ --\u003e DT --\u003e DW AC-. Provides Analytics .-\u003e Customer To understand what’s going on in this diagram, let’s assume the Machine Builder sells some sort of complex machine for manufacturing. There is a customer who buys many of these machines and runs them in factories across the world. The machines are serviced by different companies depending on location. As part of the customer’s contract requirements with the service providers, they ask that their service data be made available. The customer decides they want to provide more insight into their factory operations, so they pay an IT Service provider to collect and process all the data and make it available in a centralized data warehouse. The customer also contracts with an Analytics company to provide some sort of Analytics (e.g. Dashboards and KPIs) they can use to monitor their machines and improve operations.\nThere are lots of variations of the diagram above. Maybe the Machine Builder sells an IoT subscription where each machine automatically uploads data to a cloud service and the customer can access it. In that case, maybe the IT Services Provider is not required and the Analytics company can directly access that data.\nIt could be that the Service company and the Machine Builder are two separate companies under the same international conglomerate and they work together to provide Analytics. When the customer buys the machine they get some basic pre-built dashboards for no additional cost, but there is also a subscription that provides more comprehensive service dashboards and other features (e.g. notifications).\nHow these different companies (boxes) are organized and who offers what service can change the incentives around what data is offered. If the Machine Builder only builds and sells machines, their machines might be designed to expose the bare minimum of data. If the Machine Builder sells an IoT service, they might be motivated to collect and provide more data, but at different data subscription tiers. If the Machine Builder and the Service company are closely aligned, maybe they will have access to data that the Analytics company will not get access to.\nThe customer is not automatically passive in all this. A very large customer might say they need the machine builder to update the machines to expose some data. If the machine builder says no, the customer might start considering a different machine builder for their next factory.\nThe Incentives around Collecting Data Realistically, nobody starts with the idea of collecting and exposing as much data as possible just for the sake of doing it. Collecting and exposing data has costs in terms of the time to build the functionality, the hardware and software people needed to implement it in the machine, and the financial costs to do it. Somebody has to conclude that it’s worth it to expose that data. For a company and the business org under it, that usually means making that data available will increase profits by increasing sales or driving down costs.\nWhether we are talking about all the business functions being under a single company or across multiple companies, it’s important to realize that the incentives between different business functions motivate the decisions around what data to expose. Rather than provide generalities, I’ll provide examples.\nAn Engineering (Machine Builder) org focused their “post-sale” troubleshooting efforts on identifying the parts of the machine that were breaking and driving up warranty costs. To identify the root cause of the issue, they identified the steps (using the Error log) that led to the issue. For example, Step A -\u003e Step B -\u003e Step F is more likely to lead to issue than Step A -\u003e Step B -\u003e Step D. Another name for this is Root Cause Analysis by using a Causal Tree. By recreating this in their lab environment, they were able to identify what was going on and if it was more pragmatic to update the software/hardware or eat the warranty cost. However, the Service Org was seeing many other machine problems that could not be diagnosed using this method. They needed sensor data to help determine what was going on. But since the Engineering group felt their troubleshooting methodology was sufficient for their needs, they consistently deprioritized adding new sensor data to the machine data exports, which hindered the Service Org. An Engineering Group wanted to add a lot of telemetry to their product, but there were concerns about legal risks. After classifying their use cases as risky or less-risky, it was not clear there was a good business model for only implementing the less-risky use cases. In other words, there was not much value to the customers or the company to add that telemetry. A Service Org wanted to move from reactive (e.g. fixing an issue after it occurs) to proactive (identifying and alarming on possible issues before they occur) service. However, due to budget limits on staffing, they couldn’t realistically act on a proactive alarm in less than 96 hours. Based on this, they where not really motivated to make the business case to engineering to increase the data frequency or what data was collected (e.g. instead of just a daily average, have an hourly minimum, maximum, average, etc) In a more dysfunctional company, this is also something that can be weaponized. For example, the service org can be accused of “Not being responsive to proactive issues” even if they can’t realistically respond to them because of limited personnel. In that case, the service org might actively discourage any improvements in data collection. Instead of talking about Machine Data, here is an example from Service Data. When service personnel went onsite to a customer, they were required specify (in the service ticket) the part of the machine they worked on and what was broken. However, in many cases, it was not 100% clear what the root cause of the issue was, resulting in multiple parts being changed. So the data that was entered into the service ticket reflected what was changed, but not what the cause of the breakage was. From a Data Analysis and Predictive Model perspective, this added label noise because it was hard to determine actual reason for the issue. Fixing this would require updating the service software + UI used by the service personnel, and retraining all the service personnel on the new system. Looking at this purely from the perspective of what value cleaner data and better analytics would provide if they made this change, it was not clear that it needed to be done. As a real world example of this, there was a water pipe that was routed over a circuit board. If that pipe leaked, it would leak directly onto the circuit board and cause a failure. Since the circuit board was much more expensive than the pipe, the former was typically entered as the “cause.” This was the type of issue where implementing an engineering fix (e.g. moving the pipe) would provide much higher ROI than adding a bunch of data to predict this issue. There are machine issues where fixing something after it breaks is the best course of action. For example, there might be a 50 cent rubber gasket that leaks, but it takes 6 hours of labor to replace that gasket. In that case, it might not be worth it to collect a lot of data to try and predict when it is going to fail. Instead, you replace it every year as part of a yearly scheduled maintenance because it isn’t worth it to optimize when that part should be changed. An Engineering and Services Org was struggling to clearly define which parts should be reengineered (e.g. like a recall), which should be covered under warranty, which should be fixed by service, and which should just be left to break and fixed reactively (customers without a service plan would have to pay for it themselves). In other words, who should pay for the fix? Since they all agreed that the current confusion was untenable, it was much easier for all business owners to make the case (and find budget) for better data so they could better classify how issues should be handled. As you can see in all these examples, the goals of all the business owners define how motivated they are to push for data collection and data quality improvements. And it’s not just each business owner themselves, but these motivations are also driven by how collaborative or adversarial these relationships are between these owners. Every situation is different, and can evolve (or devolve!) over time.\nHow does this impact Analytics and Data people? As somebody who works in a data analytics role, data issues can be really frustrating and you can feel like screaming “Can’t you see this change with the data is needed? Why is it so difficult for you to understand? You are the one who asked me for this use case, and this is what needs to be done to be successful.” It’s not always clear why some data is not collected/exposed or why some major data issue is not taken seriously and addressed.\nOne thing that has helped my sanity in these situations is to defocus from the particular project I’m working on and think about the business motivations. Many times, this provides the perspective needed to understand why a change is not going to happen and the real answer is there is no business justification for resolving this. And in many cases, if a fix requires a jump from one ownership box to another, that’s one box too far.\nIf you are pushing for a change, instead of beating people over the head with more analysis and trying to convince them you are right, step back and look at motivations and business use cases. People understand “The impact of not resolving this data issue is $100K” a lot more than 50 slides of data limitations and the issues it causes. In doing so you might realize the fight you’re having isn’t worth the effort you are putting into it. I’ve certainly been in a situation where the expected ROI of a project was very high ($100K+/year), but after some analysis we realized that with the use case defined the way it was, that number was totally unrealistic and that number was revised down quite a bit and the project was put on hold.\nUnfortunately, it might take a long time to be able to see these business motivations, especially as a new employee or with new stakeholders. You also need to form the right relationships with the right people to really understand what is driving decision making. But I think it’s worth the suffering for you to build that knowledge and those relationships.\nAt this point I hope you have a basic understanding of how data flows and how different business owners can drive what data is collected and how it’s used. In my next post I’m going to provide more concrete example of machine data, and also use this to provide more examples of how business requirements drive and define what data to collect\nFinal Thoughts To wrap things up, let me just reiterate some key takeaways. Data is collected because somebody thought it would be useful to collect, not just for the sake of collecting data. In a commercial context, the people who define what is useful are business owners, who want to use this data to reduce costs or increase sales. The business owners are motivated in different ways, and it’s important to keep their motivations and incentives in mind when trying to understand what data is collected and why they find it useful. Also remember that different business owners have to work together, and how aligned their goals are influences how they perceive each other.\nOne last point. Over time, the cost of collecting data goes down, and the need to collect data goes up. A 30 year old machine might not expose any data at all, and the only way to even get data from it would be to physically add sensors and electronics to capture and export that data. In 2023, unless a machine is extremely simple, it’s likely a machine will collect some data, even if it doesn’t give customers access to that data. Compared to 30 years ago, there’s no need to convince anybody that collecting data from a machine is something that needs to be done. It’s really more a question of what data needs to be collected and how to make money from that data.\n","description":"","tags":null,"title":"Businesses Stakeholders are the ones who Define why Data Should be Collected","uri":"/posts/businesses-stakeholders-define-why-data-should-be-collected/"}]
